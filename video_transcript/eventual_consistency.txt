https://www.youtube.com/watch?v=lwIA8tsDXXE

"okay so let's get started so I am on the
intro slide so today we are we're going
to be talking about eventual consistency
and how you should embrace an optimistic
design persistence layer so today it's
going to be more of a high-level scuffle
and not really welcomed into Cassandra
internals although the eventual
consistency discussion and will center
around Cassandra a lot of these concepts
also carry over to other distributed
systems and not just databases next
slide so Who am I my name is Christos
Collins's I work for Netflix I'm the
engineering manager for a team called
cloud database engineering we manage
everything that is persistence in the
cloud and netflix today stores about
ninety-five percent of all our data in
an Apache Cassandra for those would like
to follow me on Twitter that's my
Twitter handle or if you'd like to link
with me on LinkedIn feel free to do so
and so let's get started next line so
Cassandra replication and let's do a
Cassandra replicates and inconsistency
read that cassandra is an eventually
consistent database that means your data
will get there eventually so eventual
consistency is a new term that that's
been corn the last few years basically
and it's replicated even even in your
old master slave reputation of my sequel
that was an eventually consistent model
Cassandra's model just takes it to to
multiple factors at a higher level and
what I mean is you're not just you're
not just replicating from a master to
assume you're replicating to multiple
peers at the same time and they are
replicating to you so what is it
eventually consistent I mean it doesn't
mean it data will get their day from now
it's not going to get there a minute for
more it's not even it's not even going
to get there second come in most cases
in over over ninety-nine percent of the
cases your data is being replicated in
milliseconds so I'll go to term the term
it sounds kind of scary eventual
consistency really your data is going to
get through all of your moans fairly
quickly and in the small cases where it
won't there are really remediation there
is really da shun strategies within
Cassandra to get that data everywhere
and increase the consistency even more
but so we're getting ahead of ourselves
so the fender is also also has a tunable
consistency model so yes we keep on
talking about how your data will
eventually get there but Cassandra's a
consistency level is tunable so you can
tell a statement to only return after
all the nodes have gotten have gotten
the data so in the case of a replication
factor of three so every right you want
it to be the three different locations
you can say a only return when it all
comes along well that's that's a no-no
in distributed computing you can do it
there's it's fine i'm sure there's use
cases and we'll all make sense but
you're giving up a lot of availability
and you're giving up a lot of
performance a lot of people have looked
at coram which is corn is replication
factor divided by 2 plus 1 as kind of a
sweet spot or or a a best of both worlds
if you wish fine okay I mean that still
allows you to use of the nodes in the
Cassandra cluster and still in still
achieve core
them but again you're waiting for more
than one mode to return the statement
before your application can move forward
again it's a man it's not a no but it's
not the ideal situation and then there's
replication factorable warm replication
factor of 1 I think people should
embrace it more especially with the
massive amounts of signals who are
collecting these days you know forcing
an application to wait while you well
while you're but while you're waiting
for your database to write to more than
one node seems seems a little excess to
and then we'll talk more about them a
little so back in the early 2000s crees
got some could increase dose relics are
no whip we're on slide for Cassandra
replication and consistency we can let's
go forward one slide Papa remember when
yes okay thank you sorry I need to
remember to tell you to move forward so
um in the early 2000s this model you see
was very very common you had a database
master who got all the rights and then
you you had some read slaves for either
backups or to be able to scale out your
rings the problem we're solving in the
two in the early two thousand one hey
I've got some rights here but a lot of
people want to read read this and I need
to scale or the internet so so we came
up with these architectures and so that
applications will write to a master and
read for multiple slaves
the next slide
but the slaves could lose transactions
it was a feeling that that statement
based replication and i'll use my sequel
as an example that statement based
replication was flawless and you can
never lose 12 all that's untrue I've
managed many among my sequel
quote-unquote clusters and there were
instances where the slaves lost who
didn't didn't get every transactions and
for the master now the problem with this
architecture is that if you notice all
rights are still going to the master so
all you've done is scaled out for reads
today with the Internet of Things and
all these signals being collected and
and web companies wanting to know and
collect every piece of data and that can
possibly flow through their system this
model just doesn't cut it you've got a
bottleneck the bottleneck is that one
master collecting all the rights you
want a distributed database that that
can share the responsibility of
collecting rights that's where a
distributed database like Apache
Cassandra excels because any node in the
cross will can accept right and and as a
matter of fact not only will it accept
right it will they will copy them to
other peers but then you where the
consideration is and then the the point
I'm trying to get through with this
presentation is you are extremely
limited by the throughput of of the data
you can collect if you choose a higher
consistency level during right
next slide
now let's go back to our my sequel
cluster so we said that we said that
sometimes statements would get lost now
I'm sure there's some season my sequel
dba's or names in giving in other are
dbms is in the audience today who
probably wrote their own tools or came
up with their own process to quote
unquote to repair a a a out of a
inconsistent slave from Alaska however
you know it'd be worth simple and I and
I showed here at a my sequel prompt if
you want to root their database one or
repair slave one there's no built-in
functions to do that in a in the
existing argue be a message out that the
next slide
and just to recap Cassandra and other
distributed databases do well let's talk
about the same thing Cassandra does have
a repair function so Christian just to
confirm we are now we'll select remember
when with a bunch of locals yes that is
correct Christos we are we are keeping
up actually one of the audience
participants said that these slides will
be eventually consistent and I love that
with your great fantastic so so it's
interesting I mean that pattern like I
said was trusted it worked great I mean
companies were using it and and you know
their sites were were up with a little
more effort but it worked and it still
works for a lot of companies facebook is
a huge my sequel user with with with
that master slave model and it's working
for them and they're starting to use
more and more distributed databases but
the core functionality is still running
on my sequel I stop photos zappos you
send it now called I tale Samantha these
are all companies that although are
currently researching distributed
databases and that Apache Cassandra they
are my sequel shops who are using that
master slave replication and and again
you can do it it's just so much more
effort when the product will do it for
you next slide
so Cassandra consistency concerns these
are quotes I've heard from people when I
then talked and talked to them about the
Apache Cassandra and and and
one quote said I want high consistency
in my reads rights just like I had in my
are DBMS oh well I I just showed you
that that not only can you lose can you
loop it and i just showed that not only
is the master in the slave not
necessarily one hundred percent
consistent because there is a lag from
between from when the master the master
transaction gets death sent to the slave
sometimes also statements don't get
there and I remember working at a
previous company where then we would
write to a master and we on the sleigh
but then we had to introduce asleep
between the right and read because again
it wasn't it wasn't fully consistent
between that master in that slave prez
it eventually had to replicate and and
higher higher the amount of right front
foot was bigger that sleep had to be so
I mean if you're going to scale for the
web you can't assume an extremely high
consistency level in your rights and
your reads especially if you're if
you're trying to scale out with a master
slave architecture I want my DB to catch
integrity issues if any of this audience
has ever worked with rails rails or
other MVC frameworks where you model a
the code in your model is then
translated into SQL statements to two
then run on your are DBMS you'll notice
they never create foreign teams we've
been turning off foreign keys for years
why because every time you write to a
child table before me and there's a or
include bit string linking it to a
parent table it's always it's always
doing three needs to make sure that
and that the parent ID exists that
increases latency so people dbas have
been turning boring things up for years
now to try and eat out more performance
out of their database and the next
question is can I trust that Cassandra
will replicate my data when write it
when writing at CL 1 well okay so let's
address that last one next slide so that
looks didn't experiment this was this is
about Mirabal this was on the single 117
we created 248 node clusters in two data
centers and we put load on this
Cassandra wing we put a hundred a
hundred thousand total operations or 750
thousand in each data center and then i
wrote a million records a million
records i knew of at a consistency level
of one in the first data set then i read
those say million records at consistency
wobble warm in the other data center all
in this particular toss all the records
were read successful so I mean that
means you can trust next slide so next
slide shows a graphical representation
of of the of the experiment so you have
48 nodes in each data center I was
sending 50,000 operations per second on
each side just to show that there's load
on on the cluster and in on the right
hand cluster I wrote a million records
in the left-hand cluster I read back all
those million records now your mileage
will vary this was honorable conditions
but the point I'm trying to make here is
and it's not hopeful it's I mean almost
all your rights will make it to love us
all
next slide
so all this brings us to should you be
designing optimistically or
pessimistically if you design
pessimistically and assume everything
should get there and I should cover
every single edge case well you're going
to punish ninety-nine point nine percent
of your users and what do I mean by ball
yes things will go wrong let's say point
one percent of statements the boat or
things will go wrong point one percent
of the time do you really want to punish
the rest of your customers for more than
90 day of the rest of your ninety-nine
point nine percent of your customers if
you increase the consistency layer
you're increasing your latency and
you're diminishing that user experience
which today is the most important and
I'll give you an example I use this
example quite often let's say you're on
the netflix side and you just added a
movie to your list or the interesting
tube it used to be called in three cubes
called my list more ninety-nine percent
of the time you're going to see it s
that you know you were going to add it
then you're going to look at the list
and we'll see the movie maybe one
percent and I'm being quite conservative
yeah quite liberal with this with this
with one percent of the time you're not
going to see that movie but when you
refresh it will be it is a customer
going to leave that looks because of
that I would argue know and and and and
so we've embraced eventual consistency
you
maybe maybe Chris doses audio will
become eventually consistent as well
just bear with us everyone we will try
to get crees those back I I was under
the assumption that the telephones
worked in Canada but maybe not so just
just bear with us if you have any
questions up until this point as well
can definitely can definitely try and
take some of those so we will we will
share these slight all I think we just
got a question how can we get a slide to
your sharing so we always post to planet
Cassandra the slides and the recording
from each webinar and we do that within
about 24 hours of the webinar ending so
check back tomorrow and the slides and
the recording will be there yes for
anyone asking about being able to hear
crystals you cannot hear Christa moment
as he has is been disconnected
creased i'll see you back you back in
with seeing you still still no audio
Christo's
hi Christo's can you hear us again yes
some back perfect perfect we did so I
made fun of Canada for having terrible
telephones took a took a couple of
questions from folks and the great thing
is we've got some fantastic questions
coming in we are still on optimistic
versus pessimistic designed so you were
just talking about when we lost you the
higher consistency higher latency
questions right don't make me a
presentable I can control it it seems
like every formal that turned into the
presenter that's when it freezes so if
we leave it as is it looks like i can i
can i can modify the and Google
slides forward and waffles so why don't
I just take over from here
can you hear me you so what what do you
think results we should just advance the
slides don't give you the bowl at all no
leave it as is right now I can change on
these slides and along your just ok so
you are just changing them locally when
you change them no one else will see
your slides move so you still will not
desire correct yeah you still need ok
you still need to tell us to advance
them but for the audience to see
interesting then why don't I go back to
my slides locally and let you
advancement perfect perfect so
optimistic versus pessimistic design
slide 12 is well we're at right now
great ok so let's stay on that slide so
higher consistency is higher latency
just remember that yes you can move
InDesign pessimistically but really
you're just you're just penalizing the
majority of your customers for those
edge cases where something might go
wrong and and and you're just
diminishing the user experience next
slide in an optimistic design you're
trusting your day of school and you know
your business and your application that
you're always asking yourself is it
really wrong important and you can
handle edge facing sleeping teens and
sequins so what the point here is
companies need to really relieved
understand the business and developers
need to be partners with the business
team to really understand what they're
building and and and what edge possible
edge cases can occur and how we can
handle them I understand a lot of times
engineers work as consultants and
they're just brought in to solve a
problem
and they really can't be as invested as
possible and find that happens but but
really the architects of these
applications really need to be invested
not just in the technological aspect but
William in business and so I've already
shown that you can trust Apache
Cassandra with delivering the data at
the end again your mileage may vary
depending on the quality of the network
between data centers or within a data
center however you really need to ask
yourself is it important if something
goes wrong x percent at a time and how
will you handle those edge cases so
let's talk about those education and out
of a horrible next slide so a low
consistency example one retail what I
hear quite often is I'm in retail I have
to be consistent my inventory system I
mean I can't sell widgets I won't have
I'll be a laughingstock well I believe
there's two examples where Amazon in
outlook I usually use Amazon as an
example for this but just this week my
wife bought something on Oh to look that
that she believed was available for sale
ultrabooks oltre parole yet eventually
contacted and said pun intended
eventually contacted her and said oh
we're so sorry we unfortunately do not
have that installed we are refunding you
and we're also going to give you ten
percent of the purchase price towards
credit of your virtual of a future
projects see that's that's an example of
business and engineering working
together to come up with a contingency
plan which in this case isn't even
technolog
and and in the end my wife was very
happy she said bonus they should they
should sell me things they don't have
more often get through credit towards my
next purchase you end up with a very
satisfied well you end up with a
customer that is not upset the
experience was very smooth because
because of because of a very fast
persistence layer however things did go
wrong you know and my wife might have
been v 1 million customer and at 1
million you know this issue happened
however they handle with a angled it
right let's look at another example my
favorite one I'm in banking thanks you
know banks need to be consistent you
can't not be consistent in a bank well
banks have been eventually consistent
since the beginning of time the banking
system is the ultimate is the ultimate
eventually consistent system and as you
see here I've got a quote from that file
co-founder of data stacks who says banks
make a lot of money off of eventual
consistency if I overdraft they charge
me now you know find another reason to
hate banks iPads but you know they've
embraced it and they've got a
contingency plan and unfortunate that
unfortunately for us end users their
contingency plan is the blame off and
heartless normal but the point here is
that even even in industries where where
we usually look at sacred cows of the
full consistency they've embraced
eventual consistency next line hurdles
face well it other than the
technological hurdles for today for this
presentation the hurdles faced by rule
face you audience who are convinced of
the merits of eventual consistent
see you will face hurdles and some of
that is no your engineering colleagues
or maybe stubborn I mean you know one
plus one equals two to them not
eventually too so you know you're going
to have to convince some people that
this is the right thing to do middle
management Wow so some of the son of
some of the most some of the hugest
hurdles I've ever faced work these
middle managers whose mantra is nobody
ever got fired for wine you know fill in
the blank IBM Oracle whatever look I
mean they there if that is the way they
are evaluated is uptime uptime based on
a set of throughput and and a number of
errors and and because the game is
stacked that way against them well they
see the adoption of eventual consistency
and a new technology like Apache
Cassandra as something that could
potentially lower their evaluation even
you need the middle management needs to
be convinced well it is in your best
interest to to go this role and an
embrace eventual consistency and quite
honestly some some companies are just
going to have to change the way they
they evaluate their their success and
what success means to them you need to
engage the product teams to implement
some time of content some type of
contingency pause like like I mentioned
earlier you you have to care about your
product and I'm talking to you engineers
you have to care you have to understand
the business and if you don't understand
the business and you can't explain why
you need to do this in business terms to
the product teams then that's gonna be a
huge fertile to convince them that it's
okay to sometimes sell a widget that you
don't have any swamp
next slide
so how to overcome those hurdles for the
other engineers prove it in a POC show
them that you know you can throw a whole
bunch of data to Cassandra and it'll
make it there and and and and and if it
doesn't well what percentage of it won't
make it show them the benefits of you to
expect the children that if you do it a
quorum or at all that that your
throughput will go up significantly as
you lower that consistency level i
believe google released a white paper
back in january about a super consistent
distributed database and they were able
to to achieve a throughput of the
hundreds hundreds of transactions per
second and in the conclusion of the
white paper world's we came to super
high consistency in a distributed system
and expect a group oh it's just the fact
and and lastly an after after trying to
convince engineers trying to convince
middle management and and and the
product team and we still don't get it
well maybe you're working for the wrong
company netflix is always hiring check
out jobs netflix calm next line so uh so
that's the end i think we have about 20
minutes left for questions and I'd love
to answer some of these questions and
christos we do have some questions
already the world will get started thank
you very much Chris Oakes for weathering
the technical difficulties there keeps
us on our toes likely be boring if
everything work too well I think okay so
Tim mess is asking what about the
generalized argument
use sequel server if you want
consistency or presumably any other
relational database use no sequel if you
don't care although gland it's enough to
turn off many managers and execs so um
even datastax and it talks about
polyglot polyglot persistence there is
no one solution one size fits all in the
persistence in the persistence world um
and again I get I go back to the intro
of my presentation and you know there
are going to be use cases where you need
a super high consistency and in that
case by all means use something that is
that meets your needs your compliance
needs your technological needs however
if you wherever you can adopt eventual
consistency do so because it'll just
make your usual schools all that much
about okay great I think you attract
this one in the presentation multiple
times but it's probably just what
reiterating so a netflix you know as you
work with Cassandra how long do you have
to wait until you can read the data out
of Cassandra so so it depends on the
pattern you're talking about so if
you're talking about a read-modify-write
pattern chances are chances are that's
and that's an anti-pattern first of all
in magic ascender in any distributed
database you don't want to do that
however and there is no wait you write
it and then you can read it and weep
when you write to Cassandra it is it's
not a sequential writing to the slaves
it is going there it is going to all
nodes of the replica set at the same
time
the only difference is at a lower
consistency level the coordinator is
waiting for fewer nodes to confirm the
receipt of the of the right before it
returns the statement to the application
with so in essence all the nodes got
them at the same time there is just one
node or two nodes that that responded
quicker than the other nodes by the time
your app gets that okay that success
right and goes back the data will be
there okay great of consistency
across AWS availability region example
servers in the US and Europe can you
talk about that absolutely so when I
talk data centers in the AWS world and
for those who don't middle Netflix and
runs all their Cassandra buses in AWS
across zones and across regions we do we
do replicate our clusters across
multiple regions and and it's also the
nature of our of our business where you
know yes so the data will take let's say
50 milliseconds to replicate from us is
to to EU West however and remember that
a user writing in EU wish to who's at
the edge geographically is hitting API
servers we do is never going to go
across the pond to read his or her
record so I'm still within a the active
records for that user are still within
the same region now if that user was to
do something
on a plane the travel across the
Atlantic and and then and then try and
read let's say their list the movie list
I have extremely high confidence the
data has replicated over ball I John off
is the data reside in memory on the
cluster or on disk storage oh okay so
this is more a Cassandra informal scroll
stream so data data first gross for
memory in some mem tables and then
periodically and I won't get into to all
the different things that can flush data
from from a mem table 2 to the SS tables
however periodically data will be
flushed from memory to this
ah dimitrios a couple of questions here
how many knows you run what's the data
size of a single node are you using any
dates tax enterprise features those are
all dmitry questions maybe you can group
those together absolutely so yes we we
currently are in the final stages of our
grade from 1172 datastax enterprise we
can fly which is overly viscous end
belong dot-to-dot something underneath
so yes we are running the effects
enterprise are we have 2500 plus nodes
in production across about a hundred
Cassandra pastas the average site and
i'll even give me more important our
average size cluster is 12 to 24 nodes
and our biggest cluster is 200 in really
late nodes and our smallest cluster is
three node cluster and our and from from
data center replication we do anything
from one single data center or EWS
region to as many as for AWS regions or
gives great this next one is like a
three parts or two so Jeremy asked on a
right does Cassandra know once the data
has been fully replicated one state r is
confirmed to be consistent there's a
weed of that same data with cl quorum
still need to read it more times and
then third part doesn't Cassandra know
if the data is can
distant what does caboolture know if
this data is consistent so let me add
let me answer the first one at any any
one time cassandra is not more that the
data is consistent everything is best
effort and and and so that answers that
part so there's no like master state
somewhere a keeping track of of records
are consistent and available in every
replica set the Christian can you repeat
part 1 on a right did Cassandra know
once the data has been fully replicated
great so every right to cassandra is
also passed a replication pump so if you
passed replication platteville all then
the coordinator part of the node that
your application spoke to will wait for
all replicas to have received the data
before it releases the statement back to
the application so at a high at a high
consistency level Cassandra will will
will because not cassander itself but
Cassandra organically will know that the
data example world because it wouldn't
have released the the statement back to
the application at a lower consistency
mobile as you go down forum or at at
consistency level one to center the
coordinator no the coordinator of the
node unit in your cluster will wait for
X amount of nodes to respond before
releasing the statement but there is not
a centralized kind of loan who is
considered what data is consistent and
what is not now what there is then is is
a tool called repair which it will then
go and analyze data in a mode and who
it's replicas should meet and see hey
I've got this rep you have that I'm
oversimplifying this of course a given
record eh hey all you people who should
have a popular model yes it's the one I
have the latest one no it's not oh you
have the lake swamp then it will copy
the latest one to all the three members
of the replica so that is how Cassandra
eventually gets things consistent across
the board if data was lost during normal
replication blowingly normal roles great
here we go we have a specific argument
in our business we need information
available to customers as close to real
time as possible other vendors have
stated this can't be done with a no
sequel solution due to eventual
consistency so i'm not sure that what
maybe we're mixing and matching real
time versus consistency but maybe you
can talk a little bit about that and
then the other thing i was thinking
christos is it worth talking about
lightweight transactions in in two dot
zero for example oh I'm not an expert in
the lightweight transactions available
in 20 so I don't want to venture into
that I'm sure that I'm sure you probably
have other webinars we can we can point
people to probably have a Patrick
McFadden thought you can yeah there's a
great blog post gradually on on datastax
calm about lightweight transaction so
yes no no problem no problem on that but
let's talk about this information
available to customers as close to real
time as possible because it netflix
obviously you know you're striving for
that absolutely now and without knowing
that more about the new skates
you write to Cassandra it's available
it's there and I don't know what how
much more real time how much more real
time then you write it it's there you
can read it you can get I did you know
what I'm not going to venture to i'm
probably going to stop at state it's
just a database like anyone else you
write something in there then you get it
out where eventually eventual
consistency comes out is not the speed
at which inform all accessories day well
it is it is more the be as the data is
everywhere and replicates across your
cluster how consistent will every read
do as you read it out okay great I in
the caramel Sandra you you write it you
can read it there's there's no there's
no there's no line okay thanks thank you
very much here is and then I I I know
that debate information is available in
the netflix case study I believe but
what was the main reason or features of
why you switched from a relational
database to Cassandra so oh this is that
agreeable so when we decided to go to
the fall so not which was an Oracle shop
you know we don't hide it everyone
everyone was doing it back then so are
we so but as we decided to go to the
cloud we needed something that can run
on commodity hardware that could be
distributed that could replicate across
multiple regions and and being a Java
shop we wanted more control on the on
the persistence layer we wanted to look
at the crowed we didn't want something
with the million bells and whistles we
wanted something simple that acted as a
persistence layer and then we can move
budget higher up in the applications
fall but the real so being open source
was important for being open source in
Java was great because more pictures of
java actual but the real kicker was its
multi-directional multi data center
reputation that that was and tools in it
to correct itself if need be run repairs
and make and make the cluster consistent
those were the kickers and for anyone
looking at other distributed databases
that have those needs to be able to run
in multiple data centers not in a dr
scenario but as an active active
scenario then even today cassandra is
the only one that will do it all box
yeah well boy we you know cassandra has
this reputation for you know massive
scale but more and more lately over the
last I'd say six to nine months we
actually see companies who have very
very small data sets in no way shape or
form could you say they are big data at
all but this requirement for always on
availability across multiple data
centers is a is more and more driving
force for why we see people
transitioning from relational databases
or even other master slave system in no
sequel and to Apache Cassandra so
Christos I know you looked at a bunch of
no sequel systems I don't know
separately if couchbase was one of them
that I have a couch based specific
question here you comfortable with that
or no I'd like to pass I'd like to hear
it and but now i will pass I'm not that
I'm not that versed on politics yes
meaning and it's a performance question
how is the performance compared with
couchbase and in terms of reeds right I
I don't know I don't know and if
somebody can actually share that I would
love to hear it as well and and again
and be pin and I'll say one thing and
I'll say one thing and not knowing couch
faces performance but but you're not
although Cassandra performs really
really really really well you're not
running Cassandra because of tes tes
rated transactions per second you're
running it for high availability you're
running it for scalability you're
running it you're running it because you
can embrace the eventual consistency and
have and have that huge amount of liquid
coming room Cassandra will scale
linearly we have shown whether there are
articles on on netflix is a tech blog
showing how Cassandra scales linearly so
it's comparing one node of Cassandra
with one note of couchbase or three node
updates and remote for Sandra really
you're so comparing apples to apples at
this one you're you're you're doing
yourself a disservice with your pitiful
model because you can just scale out of
a central cluster and achieve any any
sweets really bought it great how he
actually has just written in the right
availability apparently is better in
Cassandra then couchbase but yeah no
numbers I think for everyone crease or
two
point is one that should be made again
and that is really no sequel vendors are
not making this easy on you the term no
sequel is not a good one because these
systems are not all created equally so
really understand your use case and pick
the solution that matches that because
different systems have different
strengths and different weaknesses Oh
hurry is it went maybe we can copy and
paste the page and maintenance guy he's
got loads of data so well with shanna
great Thank You Harry great Gary can I
so mark is asking can I use SQL with
Cassandra Christo's I don't know what
Netflix experiences with cql is but
maybe you could just address that a
little bit so one of the one of the
major things on that that kind of
stopped Cassandra's adoption in the
early days was the API the Devi I mean
Ops guys were sold on it there was no
there was no doubt operationally it was
a better database however developers
kind of didn't like it that much you had
to write to the grip protocol it wasn't
intuitive it wasn't like SQL so that's
why other other other no sequel
databases like who had a much much
cleaner API you know won the hearts and
minds of developers early on so that's
where the Apache Cassandra community
said any booze ralston and c ql cql came
along now you can create tables khuri
tables and use a lot of the SQL syntax
apology you're familiar with today with
Cassandra
using the soup to a local place now of
course they're still no joy well you
can't do everything you could do before
but it becomes now a lot more familiar
and a lot more usable immediately usable
and with less of a learning fold than it
was say just two years ago okay great
unfortunately everyone we are top of the
power and so we we have no time for more
questions but Christos please note there
are loads more questions available maybe
we can send them to you maybe you we
through maybe there's a blog post or two
in it but please follow Christos on on
Twitter incredibly knowledgeable about
Cassandra and obviously its
implementation and Netflix if you want
to carry on your Cassandra training some
of the questions we got today are from
you know people new to Cassandra we do
have a free online training course
available and the link is on your screen
right now and then please join us for
our next webinar which is building apps
on Python with the Cassandra Python
driver Eddie sassily also a great
proponent of the Cassandra community so
Christoph's will let you get back to
your vacation have a fan tire to having
fun travel I've got a couple of
announcements actually Oh announce the
way recruit away announced away no no
not a recruit already did I already
blood that may 14th if you are in the
Bay Area the next datastax Cassandra
meetup was the South Bay Cassandra
meetup will be at Netflix and Ruffo will
be talking when you had
question before about replication across
multiple AWS regions her topic is going
to be how we achieved active active
across two three four data centers using
Apache Cassandra so for those in the
area please we will be recording it so
we will be making it available to be to
the whole community also if you are in
Montreal I will be speaking at Tuesday
the 29 at the RPM center on guy street
at 630pm it won't be this presentation
it will be a very specific proudest
Netflix used Cassandra a kind of
presentation and for people who want to
talk more tech that one that one you
might be interesting for you as well and
that's it that's all my announcements
okay well thank you very much Chris goes
we'll see you at the meetup and enjoy
the rest of your vacation thank you very
much bye everyone"